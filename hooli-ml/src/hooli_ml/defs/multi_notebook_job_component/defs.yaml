type: hooli_ml.components.DatabricksMultiNotebookJobComponent
template_vars_module: .get_env

attributes:
  job_name_prefix: "feature_engineering_multi_job"
  serverless: true
  # uncomment if serverless is false
  # cluster_config:
  #   spark_version: "13.3.x-scala2.12"
  #   node_type_id: "i3.xlarge"
  #   num_workers: 1
  
  tasks:
    - task_key: "feature_engineering_pickup"
      notebook_path: "/Users/christian@dagsterlabs.com/.bundle/databricks_mlops/dev/files/feature_engineering/notebooks/GenerateAndWriteFeatures"
      parameters:
        model_type: "RandomForest"
        training_data_path: "/databricks-datasets/nyctaxi-with-zipcodes/subsampled"
        experiment_name: "/{{ env }}-databricks_mlops-experiment"
        model_name: "{{ env }}.databricks_mlops.databricks_mlops-model"
        pickup_features_table: "{{ env }}.databricks_mlops.trip_pickup_features"
        dropoff_features_table: "{{ env }}.databricks_mlops.trip_dropoff_features"
        timestamp_column: "tpep_pickup_datetime"
        output_table_name: "{{ env }}.databricks_mlops.trip_pickup_features"
        features_transform_module: "pickup_features"
        primary_keys: "zip"
      asset_specs:
        - key: feature_engineering_pickup
          description: "Pickup location feature engineering from taxi data"
          kinds:
            - "feature_engineering"
            - "notebook"
        
        # - key: pickup_feature_summary
        #   description: "Summary statistics for pickup features"
        #   kinds:
        #     - databricks
        #     - analytics
        #     - feature_engineering
        #   # skippable defaults to true (not specified)
        
      #   - key: pickup_feature_metrics
      #     description: "Quality metrics for pickup features"
      #     kinds:
      #       - databricks
      #       - metrics
      #     skippable: false  # Override default
    
    # Task 2: Single asset spec with minimal overrides
    - task_key: "feature_engineering_dropoff"
      notebook_path: "/Users/christian@dagsterlabs.com/.bundle/databricks_mlops/dev/files/feature_engineering/notebooks/GenerateAndWriteFeatures"
      asset_specs:
        - key: feature_engineering_dropoff
          description: "Dropoff location feature engineering from taxi data"
          # Completely uses defaults: kinds=["databricks"], skippable=true
      parameters:
        model_type: "RandomForest"
        training_data_path: "/databricks-datasets/nyctaxi-with-zipcodes/subsampled"
        experiment_name: "/{{ env }}-databricks_mlops-experiment"
        model_name: "{{ env }}.databricks_mlops.databricks_mlops-model"
        pickup_features_table: "{{ env }}.databricks_mlops.trip_pickup_features"
        dropoff_features_table: "{{ env }}.databricks_mlops.trip_dropoff_features"
        timestamp_column: "tpep_dropoff_datetime"
        output_table_name: "{{ env }}.databricks_mlops.trip_dropoff_features"
        features_transform_module: "dropoff_features"
        primary_keys: "zip"
    
    - task_key: "feature_validation"
      notebook_path: "/Users/christian@dagsterlabs.com/.bundle/databricks_mlops/dev/files/feature_engineering/notebooks/ValidateFeatures"
      # No asset_specs section - defaults to:
      # - key: "feature_validation" (same as task_key)
      # - kinds: ["databricks"]
      # - skippable: true
      # - description: "Asset for task feature_validation"
      parameters:
        validation_mode: "comprehensive"
        output_report_path: "{{ env }}.databricks_mlops.feature_validation_report"
      asset_specs:
        - key: feature_validation
          description: "Asset for task feature_validation"
          kinds:
            - databricks
          deps: ["feature_engineering_pickup", "feature_engineering_dropoff"]
          # skippable: true (default)

    - task_key: "model_training"
      notebook_path: "/Users/christian@dagsterlabs.com/.bundle/databricks_mlops/dev/files/training/notebooks/TrainModel"
      parameters:
        training_data_path: "/databricks-datasets/nyctaxi-with-zipcodes/subsampled"
        experiment_name: "/{{ env }}-databricks_mlops-experiment"
        model_name: "{{env }}.databricks_mlops.databricks_mlops-model"
        pickup_features_table: "{{ env }}.databricks_mlops.trip_pickup_features"
        dropoff_features_table: "{{ env }}.databricks_mlops.trip_dropoff_features"
      asset_specs:
        # Critical model artifact - not skippable
        - key: model_training
          description: "Trained ML model artifact"
          kinds:
            - databricks
            - ml_model
          deps: ["feature_engineering_pickup", "feature_engineering_dropoff"]
          skippable: false

    - task_key: "model_validation"
      notebook_path: "/Users/christian@dagsterlabs.com/.bundle/databricks_mlops/dev/files/training/notebooks/ValidateModel"
      parameters:
        training_data_path: "/databricks-datasets/nyctaxi-with-zipcodes/subsampled"
        run_mode: "dry_run"
        enable_baseline_comparison: false
        validation_input: "SELECT * FROM delta.`dbfs:/databricks-datasets/nyctaxi-with-zipcodes/subsampled`"
        model_type: "regressor"
        targets: "fare_amount"
        custom_metrics_loader_function: "custom_metrics"
        validation_thresholds_loader_function: "validation_thresholds"
        evaluator_config_loader_function: "evaluator_config"
      asset_specs:
        - key: model_validation
          description: "Model validation results"
          kinds:
            - databricks
            - ml_model
          deps: ["feature_engineering_pickup", "feature_engineering_dropoff", "model_training"]
    
    - task_key: "model_deploy"
      notebook_path: "/Users/christian@dagsterlabs.com/.bundle/databricks_mlops/dev/files/deployment/model_deployment/notebooks/ModelDeployment"
      asset_specs:
        - key: model_deploy
          description: "Model deployment results"
          kinds:
            - databricks
            - ml_model
          deps: ["feature_engineering_pickup", "feature_engineering_dropoff", "model_training", "model_validation"]
      parameters:
        env: "{{ env }}"

    - task_key: "batch_inference"
      notebook_path: "/Users/christian@dagsterlabs.com/.bundle/databricks_mlops/dev/files/deployment/batch_inference/notebooks/BatchInference"
      asset_specs:
        - key: batch_inference
          description: "Batch inference results"
          kinds:
            - databricks
            - ml_model
          deps: ["feature_engineering_pickup", "feature_engineering_dropoff", "model_training", "model_validation"]
      parameters:
        model_type: "RandomForest"
        input_table_name: "{{ env }}.databricks_mlops.feature_store_inference_input"
        output_table_name: "{{ env }}.databricks_mlops.predictions"
    
    - task_key: "monitoring"
      notebook_path: "/Users/christian@dagsterlabs.com/.bundle/databricks_mlops/dev/files/monitoring/notebooks/MonitoredMetricViolationCheck"
      asset_specs:
        - key: monitoring
          description: "Monitoring job"
          kinds:
            - databricks
            - monitoring
          deps: ["batch_inference"]
      parameters:
        env: "{{ env }}"
        table_name_under_monitor: "{{ env }}.databricks_mlops.predictions"
        metric_to_monitor: "root_mean_squared_error"
        metric_violation_threshold: 100
        num_evaluation_windows: 5
        num_violation_windows: 2
# Example: Using libraries in DatabricksMultiNotebookJobComponent
# This shows how to configure different types of libraries for tasks

type: hooli_ml.components.DatabricksMultiNotebookJobComponent
template_vars_module: .template_vars

attributes:
  job_name_prefix: "libraries_example"
  serverless: false
  spark_version: "13.3.x-scala2.12"
  node_type_id: "i3.xlarge"
  num_workers: 2
  
  tasks:
    # Example 1: Notebook task with Python wheel library
    - task_key: "notebook_with_wheel"
      notebook_path: "/Users/{{ workspace_user }}/ml_pipelines/feature_engineering"
      parameters:
        input_table: "{{ env }}.raw.customer_data"
        output_table: "{{ env }}.features.customer_features"
        feature_date: "{{ ds }}"
      libraries:
        - whl: "dbfs:/FileStore/wheels/my_custom_package-1.0.0-py3-none-any.whl"
        - pypi:
            package: "pandas"
            version: "2.0.0"
        - pypi:
            package: "scikit-learn"
            version: "1.3.0"
      asset_specs:
        - key: customer_features
          description: "Engineered customer features with custom libraries"
          kinds: ["databricks", "notebook", "features"]
    
    # Example 2: Python wheel task with Maven and PyPI libraries
    - task_key: "wheel_task_with_libraries"
      python_wheel_task:
        package_name: "ml_processing_package"
        entry_point: "process_data"
      parameters:
        input_path: "/dbfs/data/raw/"
        output_path: "/dbfs/data/processed/"
        config_file: "/dbfs/config/processing_config.json"
      libraries:
        - whl: "dbfs:/FileStore/wheels/ml_processing-2.1.0-py3-none-any.whl"
        - maven:
            coordinates: "org.apache.spark:spark-sql_2.12:3.4.0"
            repo: "https://repo1.maven.org/maven2/"
        - maven:
            coordinates: "com.databricks:spark-avro_2.12:4.0.0"
        - pypi:
            package: "numpy"
            version: "1.24.0"
        - pypi:
            package: "matplotlib"
            version: "3.7.0"
      asset_specs:
        - key: processed_data
          description: "Data processed with ML libraries"
          kinds: ["databricks", "wheel", "processing"]
    
    # Example 3: Spark JAR task with JAR and Maven libraries
    - task_key: "spark_jar_with_libraries"
      spark_jar_task:
        main_class_name: "com.company.etl.DataProcessor"
      parameters:
        input_table: "{{ env }}.raw.transactions"
        output_table: "{{ env }}.processed.transactions"
        processing_date: "{{ ds }}"
      libraries:
        - jar: "dbfs:/FileStore/jars/data_processor-1.0.0.jar"
        - maven:
            coordinates: "org.apache.spark:spark-streaming_2.12:3.4.0"
        - maven:
            coordinates: "com.amazonaws:aws-java-sdk-s3:1.12.261"
            exclusions: ["com.fasterxml.jackson.core:jackson-core"]
        - maven:
            coordinates: "org.postgresql:postgresql:42.6.0"
      asset_specs:
        - key: processed_transactions
          description: "Processed transaction data"
          kinds: ["databricks", "jar", "etl"]
    
    # Example 4: Notebook task with R libraries (CRAN)
    - task_key: "r_notebook_with_libraries"
      notebook_path: "/Users/{{ workspace_user }}/r_analysis/statistical_analysis"
      parameters:
        data_table: "{{ env }}.features.customer_features"
        output_path: "/dbfs/results/statistical_analysis/"
        analysis_date: "{{ ds }}"
      libraries:
        - cran:
            package: "dplyr"
            repo: "https://cran.r-project.org"
        - cran:
            package: "ggplot2"
        - cran:
            package: "caret"
        - pypi:
            package: "rpy2"
            version: "3.5.0"
      asset_specs:
        - key: statistical_analysis
          description: "Statistical analysis results using R"
          kinds: ["databricks", "notebook", "analysis"]
          deps: ["customer_features"]
    
    # Example 5: Run existing job with libraries
    - task_key: "existing_job_with_libraries"
      job_id: 123456789
      job_parameters:
        input_table: "{{ env }}.processed.transactions"
        output_table: "{{ env }}.analytics.daily_summary"
        summary_date: "{{ ds }}"
      libraries:
        - whl: "dbfs:/FileStore/wheels/analytics_package-1.2.0-py3-none-any.whl"
        - pypi:
            package: "plotly"
            version: "5.15.0"
        - pypi:
            package: "seaborn"
            version: "0.12.0"
      asset_specs:
        - key: daily_summary
          description: "Daily analytics summary"
          kinds: ["databricks", "job", "analytics"]
          deps: ["processed_transactions"]

---
# Alternative: Simple library example
type: hooli_ml.components.DatabricksMultiNotebookJobComponent
template_vars_module: .template_vars

attributes:
  job_name_prefix: "simple_libraries"
  serverless: true
  
  tasks:
    # Simple notebook with just a few libraries
    - task_key: "simple_notebook"
      notebook_path: "/Users/{{ workspace_user }}/simple_analysis"
      parameters:
        data_path: "/dbfs/data/input/"
        output_path: "/dbfs/data/output/"
      libraries:
        - whl: "${var.lib_file_path}"  # Using template variable
        - pypi:
            package: "pandas"
        - pypi:
            package: "numpy"
      asset_specs:
        - key: simple_analysis
          description: "Simple data analysis" 
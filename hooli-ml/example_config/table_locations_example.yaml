# Example: Table location metadata in DatabricksMultiNotebookJobComponent
# This shows how table location information is automatically extracted and included in metadata

type: hooli_ml.components.DatabricksMultiNotebookJobComponent
template_vars_module: .template_vars

attributes:
  job_name_prefix: "table_locations_example"
  serverless: false
  spark_version: "13.3.x-scala2.12"
  node_type_id: "i3.xlarge"
  num_workers: 2
  
  # Common configuration for all tasks
  common_config:
    - "--experiment_settings={{ experiment_settings }}"
    - "--catalog_map={{ catalog_map }}"
    - "--log_level={{ log_level }}"
    - "--llm_calls={{ llm_calls }}"
    - "--git_sha={{ git_sha }}"
    - "--output_bucket_name={{ output_bucket_name }}"
    - "--run_id={{ orchestrator_job_run_id }}"
    - "--module=ml_etl.projects.tune.tasks.test_task"
    - "--class=TestTask"
    - "--environment={{ environment }}"
    - "--profiler_enabled={{ profiler_enabled }}"
  
  tasks:
    # Example 1: Notebook task with table locations in parameters
    - task_key: "data_processing_notebook"
      notebook_path: "/Users/{{ workspace_user }}/ml_pipelines/data_processing"
      parameters:
        input_table: "{{ env }}.raw.customer_data"
        output_table: "{{ env }}.processed.customer_data"
        processing_date: "{{ ds }}"
        input_path: "/dbfs/data/raw/customers/"
        output_path: "/dbfs/data/processed/customers/"
        data_location: "s3://my-bucket/data/"
        source_table: "{{ env }}.raw.transactions"
        target_table: "{{ env }}.processed.transactions"
        custom_parameter: "task_specific_value"
      job_parameters:
        - name: orchestrator_job_run_id
          default: "{{ run_id }}"
        - name: environment
          default: "{{ env }}"
        - name: output_bucket_name
          default: "{{ var.output_bucket_name }}"
        - name: profiler_enabled
          default: false
        - name: input_table_location
          default: "{{ env }}.raw.customer_data"
        - name: output_table_location
          default: "{{ env }}.processed.customer_data"
      libraries:
        - whl: "dbfs:/FileStore/wheels/data_processing-1.0.0-py3-none-any.whl"
        - pypi:
            package: "pandas"
            version: "2.0.0"
      asset_specs:
        - key: processed_customer_data
          description: "Processed customer data with table location metadata"
          kinds: ["databricks", "notebook", "processing"]
          table_location: "{{ env }}.processed.customer_data"
          data_location: "/dbfs/data/processed/customers/"
    
    # Example 2: Python wheel task with table locations
    - task_key: "feature_engineering_wheel"
      python_wheel_task:
        package_name: "ml_processing_package"
        entry_point: "engineer_features"
      parameters:
        input_path: "/dbfs/data/processed/"
        output_path: "/dbfs/data/features/"
        input_table: "{{ env }}.processed.customer_data"
        output_table: "{{ env }}.features.customer_features"
        feature_version: "v2.0"
        source_location: "s3://my-bucket/processed/"
        target_location: "s3://my-bucket/features/"
        module: "custom.feature.engineering"
      job_parameters:
        orchestrator_job_run_id: "{{ run_id }}"
        environment: "{{ env }}"
        output_bucket_name: "{{ var.output_bucket_name }}"
        profiler_enabled: false
        batch_size: 1000
        timeout_minutes: 30
        source_table_location: "{{ env }}.processed.customer_data"
        target_table_location: "{{ env }}.features.customer_features"
      libraries:
        - whl: "dbfs:/FileStore/wheels/ml_processing-2.1.0-py3-none-any.whl"
        - maven:
            coordinates: "org.apache.spark:spark-sql_2.12:3.4.0"
            repo: "https://repo1.maven.org/maven2/"
        - pypi:
            package: "scikit-learn"
            version: "1.3.0"
      asset_specs:
        - key: customer_features
          description: "Engineered customer features with table location metadata"
          kinds: ["databricks", "wheel", "features"]
          deps: ["processed_customer_data"]
          table_location: "{{ env }}.features.customer_features"
          data_location: "/dbfs/data/features/"
    
    # Example 3: Spark JAR task with table locations
    - task_key: "spark_processing_jar"
      spark_jar_task:
        main_class_name: "com.company.etl.DataProcessor"
      parameters:
        input_table: "{{ env }}.raw.transactions"
        output_table: "{{ env }}.processed.transactions"
        processing_date: "{{ ds }}"
        input_path: "/dbfs/data/raw/transactions/"
        output_path: "/dbfs/data/processed/transactions/"
        source_location: "s3://my-bucket/raw/"
        target_location: "s3://my-bucket/processed/"
        batch_size: 1000
        timeout_minutes: 30
      job_parameters:
        - name: orchestrator_job_run_id
          default: "{{ run_id }}"
        - name: environment
          default: "{{ env }}"
        - name: output_bucket_name
          default: "{{ var.output_bucket_name }}"
        - name: profiler_enabled
          default: false
        - name: spark_executor_memory
          default: "4g"
        - name: spark_driver_memory
          default: "2g"
        - name: source_table_location
          default: "{{ env }}.raw.transactions"
        - name: target_table_location
          default: "{{ env }}.processed.transactions"
      libraries:
        - jar: "dbfs:/FileStore/jars/data_processor-1.0.0.jar"
        - maven:
            coordinates: "org.apache.spark:spark-streaming_2.12:3.4.0"
        - maven:
            coordinates: "com.amazonaws:aws-java-sdk-s3:1.12.261"
            exclusions: ["com.fasterxml.jackson.core:jackson-core"]
      asset_specs:
        - key: processed_transactions
          description: "Processed transaction data with table location metadata"
          kinds: ["databricks", "jar", "etl"]
          table_location: "{{ env }}.processed.transactions"
          data_location: "/dbfs/data/processed/transactions/"
    
    # Example 4: Run existing job with table locations
    - task_key: "existing_job_with_table_locations"
      job_id: 123456789
      job_parameters:
        input_table: "{{ env }}.processed.transactions"
        output_table: "{{ env }}.analytics.daily_summary"
        summary_date: "{{ ds }}"
        input_path: "/dbfs/data/processed/transactions/"
        output_path: "/dbfs/data/analytics/summary/"
        source_location: "s3://my-bucket/processed/"
        target_location: "s3://my-bucket/analytics/"
        orchestrator_job_run_id: "{{ run_id }}"
        environment: "{{ env }}"
        output_bucket_name: "{{ var.output_bucket_name }}"
        profiler_enabled: false
      asset_specs:
        - key: daily_summary
          description: "Daily analytics summary with table location metadata"
          kinds: ["databricks", "job", "analytics"]
          deps: ["processed_transactions"]
          table_location: "{{ env }}.analytics.daily_summary"
          data_location: "/dbfs/data/analytics/summary/"

---
# Example: How to access table location metadata in Python code
# This shows how to access the table location information

# In your Dagster code, you can access table location metadata like this:
# from dagster import AssetExecutionContext
# 
# def my_asset(context: AssetExecutionContext):
#     # Access asset spec metadata (available at definition time)
#     asset_spec_metadata = context.asset_def.metadata
#     
#     # Access execution metadata (available at runtime)
#     execution_metadata = context.get_output_metadata()
#     
#     # Example table location metadata access:
#     table_locations = asset_spec_metadata.get("table_locations", {})
#     
#     # Common table location fields:
#     input_table = table_locations.get("input_table")
#     output_table = table_locations.get("output_table")
#     input_path = table_locations.get("input_path")
#     output_path = table_locations.get("output_path")
#     data_location = table_locations.get("data_location")
#     source_table = table_locations.get("source_table")
#     target_table = table_locations.get("target_table")
#     
#     # Job parameter table locations:
#     job_param_input = table_locations.get("job_param_input_table_location")
#     job_param_output = table_locations.get("job_param_output_table_location")
#     
#     # Asset spec table locations:
#     asset_spec_table = table_locations.get("asset_spec_table_location")
#     asset_spec_data = table_locations.get("asset_spec_data_location")
#     
#     print(f"Input table: {input_table}")
#     print(f"Output table: {output_table}")
#     print(f"Input path: {input_path}")
#     print(f"Output path: {output_path}")
#     print(f"Data location: {data_location}")
#     print(f"Asset spec table: {asset_spec_table}")
#     print(f"Asset spec data: {asset_spec_data}")
#     
#     # You can use these locations to:
#     # - Validate data exists before processing
#     # - Check data quality at source locations
#     # - Verify output was written to expected locations
#     # - Build data lineage information
#     # - Create data catalog entries 
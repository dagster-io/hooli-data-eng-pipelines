# Example: Accessing metadata in DatabricksMultiNotebookJobComponent
# This shows how to access comprehensive metadata for Databricks-like experience

type: hooli_ml.components.DatabricksMultiNotebookJobComponent
template_vars_module: .template_vars

attributes:
  job_name_prefix: "metadata_example"
  serverless: false
  spark_version: "13.3.x-scala2.12"
  node_type_id: "i3.xlarge"
  num_workers: 2
  
  # Common configuration for all tasks
  common_config:
    - "--experiment_settings={{ experiment_settings }}"
    - "--catalog_map={{ catalog_map }}"
    - "--log_level={{ log_level }}"
    - "--llm_calls={{ llm_calls }}"
    - "--git_sha={{ git_sha }}"
    - "--output_bucket_name={{ output_bucket_name }}"
    - "--run_id={{ orchestrator_job_run_id }}"
    - "--module=ml_etl.projects.tune.tasks.test_task"
    - "--class=TestTask"
    - "--environment={{ environment }}"
    - "--profiler_enabled={{ profiler_enabled }}"
  
  tasks:
    # Example 1: Notebook task with comprehensive metadata
    - task_key: "data_processing_notebook"
      notebook_path: "/Users/{{ workspace_user }}/ml_pipelines/data_processing"
      parameters:
        input_table: "{{ env }}.raw.customer_data"
        output_table: "{{ env }}.processed.customer_data"
        processing_date: "{{ ds }}"
        custom_parameter: "task_specific_value"
      job_parameters:
        - name: orchestrator_job_run_id
          default: "{{ run_id }}"
        - name: environment
          default: "{{ env }}"
        - name: output_bucket_name
          default: "{{ var.output_bucket_name }}"
        - name: profiler_enabled
          default: false
      libraries:
        - whl: "dbfs:/FileStore/wheels/data_processing-1.0.0-py3-none-any.whl"
        - pypi:
            package: "pandas"
            version: "2.0.0"
        - pypi:
            package: "numpy"
            version: "1.24.0"
      asset_specs:
        - key: processed_customer_data
          description: "Processed customer data with comprehensive metadata"
          kinds: ["databricks", "notebook", "processing"]
    
    # Example 2: Python wheel task with metadata
    - task_key: "feature_engineering_wheel"
      python_wheel_task:
        package_name: "ml_processing_package"
        entry_point: "engineer_features"
      parameters:
        input_path: "/dbfs/data/processed/"
        output_path: "/dbfs/data/features/"
        feature_version: "v2.0"
        module: "custom.feature.engineering"
      job_parameters:
        orchestrator_job_run_id: "{{ run_id }}"
        environment: "{{ env }}"
        output_bucket_name: "{{ var.output_bucket_name }}"
        profiler_enabled: false
        batch_size: 1000
        timeout_minutes: 30
      libraries:
        - whl: "dbfs:/FileStore/wheels/ml_processing-2.1.0-py3-none-any.whl"
        - maven:
            coordinates: "org.apache.spark:spark-sql_2.12:3.4.0"
            repo: "https://repo1.maven.org/maven2/"
        - pypi:
            package: "scikit-learn"
            version: "1.3.0"
      asset_specs:
        - key: customer_features
          description: "Engineered customer features with metadata"
          kinds: ["databricks", "wheel", "features"]
          deps: ["processed_customer_data"]
    
    # Example 3: Spark JAR task with metadata
    - task_key: "spark_processing_jar"
      spark_jar_task:
        main_class_name: "com.company.etl.DataProcessor"
      parameters:
        input_table: "{{ env }}.raw.transactions"
        output_table: "{{ env }}.processed.transactions"
        processing_date: "{{ ds }}"
        batch_size: 1000
        timeout_minutes: 30
      job_parameters:
        - name: orchestrator_job_run_id
          default: "{{ run_id }}"
        - name: environment
          default: "{{ env }}"
        - name: output_bucket_name
          default: "{{ var.output_bucket_name }}"
        - name: profiler_enabled
          default: false
        - name: spark_executor_memory
          default: "4g"
        - name: spark_driver_memory
          default: "2g"
      libraries:
        - jar: "dbfs:/FileStore/jars/data_processor-1.0.0.jar"
        - maven:
            coordinates: "org.apache.spark:spark-streaming_2.12:3.4.0"
        - maven:
            coordinates: "com.amazonaws:aws-java-sdk-s3:1.12.261"
            exclusions: ["com.fasterxml.jackson.core:jackson-core"]
      asset_specs:
        - key: processed_transactions
          description: "Processed transaction data with metadata"
          kinds: ["databricks", "jar", "etl"]
    
    # Example 4: Run existing job with metadata
    - task_key: "existing_job_with_metadata"
      job_id: 123456789
      job_parameters:
        input_table: "{{ env }}.processed.transactions"
        output_table: "{{ env }}.analytics.daily_summary"
        summary_date: "{{ ds }}"
        orchestrator_job_run_id: "{{ run_id }}"
        environment: "{{ env }}"
        output_bucket_name: "{{ var.output_bucket_name }}"
        profiler_enabled: false
      asset_specs:
        - key: daily_summary
          description: "Daily analytics summary with metadata"
          kinds: ["databricks", "job", "analytics"]
          deps: ["processed_transactions"]

---
# Example: How to access metadata in Python code
# This shows how to access the comprehensive metadata

# In your Dagster code, you can access metadata like this:
# from dagster import AssetExecutionContext
# 
# def my_asset(context: AssetExecutionContext):
#     # Access asset spec metadata (available at definition time)
#     asset_spec_metadata = context.asset_def.metadata
#     
#     # Access execution metadata (available at runtime)
#     execution_metadata = context.get_output_metadata()
#     
#     # Example metadata access:
#     task_key = asset_spec_metadata.get("task_key")
#     task_type = asset_spec_metadata.get("task_type")
#     notebook_path = asset_spec_metadata.get("notebook_path")
#     compute_config = asset_spec_metadata.get("compute_config")
#     libraries = asset_spec_metadata.get("libraries")
#     job_parameters = asset_spec_metadata.get("job_parameters")
#     common_config = asset_spec_metadata.get("common_config")
#     
#     # Execution metadata
#     job_run_id = execution_metadata.get("databricks_job_run_id")
#     job_run_url = execution_metadata.get("databricks_job_run_url")
#     selected_tasks = execution_metadata.get("selected_tasks")
#     
#     print(f"Task: {task_key} ({task_type})")
#     print(f"Notebook: {notebook_path}")
#     print(f"Compute: {compute_config}")
#     print(f"Libraries: {libraries}")
#     print(f"Job Run ID: {job_run_id}")
#     print(f"Job Run URL: {job_run_url}") 
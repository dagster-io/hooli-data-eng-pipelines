# Example: Using common configuration in DatabricksMultiNotebookJobComponent
# This shows how to configure common parameters that apply to all tasks

type: hooli_ml.components.DatabricksMultiNotebookJobComponent
template_vars_module: .template_vars

attributes:
  job_name_prefix: "common_config_example"
  serverless: false
  spark_version: "13.3.x-scala2.12"
  node_type_id: "i3.xlarge"
  num_workers: 2
  
  # Common configuration that applies to all tasks
  common_config:
    - "--experiment_settings={{ experiment_settings }}"
    - "--catalog_map={{ catalog_map }}"
    - "--log_level={{ log_level }}"
    - "--llm_calls={{ llm_calls }}"
    - "--git_sha={{ git_sha }}"
    - "--output_bucket_name={{ output_bucket_name }}"
    - "--run_id={{ orchestrator_job_run_id }}"
    - "--module=ml_etl.projects.tune.tasks.test_task"
    - "--class=TestTask"
    - "--environment={{ environment }}"
    - "--profiler_enabled={{ profiler_enabled }}"
  
  tasks:
    # Example 1: Notebook task with common config
    - task_key: "data_processing_notebook"
      notebook_path: "/Users/{{ workspace_user }}/ml_pipelines/data_processing"
      parameters:
        input_table: "{{ env }}.raw.customer_data"
        output_table: "{{ env }}.processed.customer_data"
        processing_date: "{{ ds }}"
        # Task-specific parameters will override common config
        custom_parameter: "task_specific_value"
      asset_specs:
        - key: processed_customer_data
          description: "Processed customer data with common config"
          kinds: ["databricks", "notebook", "processing"]
    
    # Example 2: Python wheel task with common config
    - task_key: "feature_engineering_wheel"
      python_wheel_task:
        package_name: "ml_processing_package"
        entry_point: "engineer_features"
      parameters:
        input_path: "/dbfs/data/processed/"
        output_path: "/dbfs/data/features/"
        feature_version: "v2.0"
        # This will override the common --module parameter
        module: "custom.feature.engineering"
      asset_specs:
        - key: customer_features
          description: "Engineered customer features with common config"
          kinds: ["databricks", "wheel", "features"]
          deps: ["processed_customer_data"]
    
    # Example 3: Spark JAR task with common config
    - task_key: "spark_processing_jar"
      spark_jar_task:
        main_class_name: "com.company.etl.DataProcessor"
      parameters:
        input_table: "{{ env }}.raw.transactions"
        output_table: "{{ env }}.processed.transactions"
        processing_date: "{{ ds }}"
        # Task-specific parameters
        batch_size: 1000
        timeout_minutes: 30
      asset_specs:
        - key: processed_transactions
          description: "Processed transaction data with common config"
          kinds: ["databricks", "jar", "etl"]
    
    # Example 4: Run existing job with common config
    - task_key: "existing_job_with_common_config"
      job_id: 123456789
      job_parameters:
        input_table: "{{ env }}.processed.transactions"
        output_table: "{{ env }}.analytics.daily_summary"
        summary_date: "{{ ds }}"
        # These are passed to the existing job
        orchestrator_job_run_id: "{{ run_id }}"
        environment: "{{ env }}"
      asset_specs:
        - key: daily_summary
          description: "Daily analytics summary with common config"
          kinds: ["databricks", "job", "analytics"]
          deps: ["processed_transactions"]

---
# Alternative: Simple common config example
type: hooli_ml.components.DatabricksMultiNotebookJobComponent
template_vars_module: .template_vars

attributes:
  job_name_prefix: "simple_common_config"
  serverless: true
  
  # Minimal common configuration
  common_config:
    - "--environment={{ environment }}"
    - "--run_id={{ run_id }}"
    - "--output_bucket_name={{ output_bucket_name }}"
    - "--profiler_enabled=false"
    - "--log_level=INFO"
  
  tasks:
    # Simple notebook with common config
    - task_key: "simple_notebook"
      notebook_path: "/Users/{{ workspace_user }}/simple_analysis"
      parameters:
        data_path: "/dbfs/data/input/"
        output_path: "/dbfs/data/output/"
        # Task-specific parameter
        custom_setting: "task_value"
      asset_specs:
        - key: simple_analysis
          description: "Simple data analysis with common config"

---
# Example: Common config with boolean flags
type: hooli_ml.components.DatabricksMultiNotebookJobComponent
template_vars_module: .template_vars

attributes:
  job_name_prefix: "boolean_flags_example"
  serverless: false
  spark_version: "13.3.x-scala2.12"
  node_type_id: "i3.xlarge"
  num_workers: 2
  
  # Common configuration with boolean flags
  common_config:
    - "--experiment_settings={{ experiment_settings }}"
    - "--catalog_map={{ catalog_map }}"
    - "--log_level={{ log_level }}"
    - "--llm_calls={{ llm_calls }}"
    - "--git_sha={{ git_sha }}"
    - "--output_bucket_name={{ output_bucket_name }}"
    - "--run_id={{ orchestrator_job_run_id }}"
    - "--module=ml_etl.projects.tune.tasks.test_task"
    - "--class=TestTask"
    - "--environment={{ environment }}"
    - "--profiler_enabled={{ profiler_enabled }}"
    - "--debug"  # Boolean flag (no value)
    - "--verbose"  # Boolean flag (no value)
    - "--dry_run"  # Boolean flag (no value)
  
  tasks:
    # Notebook with boolean flags in common config
    - task_key: "debug_notebook"
      notebook_path: "/Users/{{ workspace_user }}/ml_pipelines/debug_analysis"
      parameters:
        input_table: "{{ env }}.raw.customer_data"
        output_table: "{{ env }}.debug.customer_data"
        processing_date: "{{ ds }}"
        # Override common config
        debug: false  # This will override the common --debug flag
      asset_specs:
        - key: debug_customer_data
          description: "Debug analysis with boolean flags"
          kinds: ["databricks", "notebook", "debug"] 
# Example: Jobs with job-level parameters and job parameter references
# This demonstrates how the scaffolder extracts job-level parameters and preserves references

resources:
  jobs:
    ml_pipeline_job:
      name: "ML Pipeline Job with Job Parameters"
      
      # Job-level parameters that will be extracted by the scaffolder
      parameters:
        environment: "{{ env }}"
        orchestrator_job_run_id: "{{ run_id }}"
        output_bucket_name: "{{ var.output_bucket_name }}"
        profiler_enabled: false
        experiment_settings: "mlflow://databricks"
        catalog_map: "{{ env }}.catalog"
        log_level: "INFO"
        llm_calls: "enabled"
        git_sha: "{{ git_sha }}"
        batch_size: 1000
        timeout_minutes: 30
      
      tasks:
        # Example 1: Notebook task with job parameter references
        - task_key: data_processing_notebook
          notebook_task:
            notebook_path: "/Users/{{ workspace_user }}/ml_pipelines/data_processing"
            base_parameters:
              input_table: "{{ env }}.raw.customer_data"
              output_table: "{{ env }}.processed.customer_data"
              processing_date: "{{ ds }}"
              # Reference job-level parameters using {{job.parameters.param_name}} syntax
              environment: "{{job.parameters.environment}}"
              run_id: "{{job.parameters.orchestrator_job_run_id}}"
              output_bucket: "{{job.parameters.output_bucket_name}}"
              profiler_enabled: "{{job.parameters.profiler_enabled}}"
              experiment_settings: "{{job.parameters.experiment_settings}}"
              catalog_map: "{{job.parameters.catalog_map}}"
              log_level: "{{job.parameters.log_level}}"
              llm_calls: "{{job.parameters.llm_calls}}"
              git_sha: "{{job.parameters.git_sha}}"
              batch_size: "{{job.parameters.batch_size}}"
              timeout_minutes: "{{job.parameters.timeout_minutes}}"
          libraries:
            - whl: "dbfs:/FileStore/wheels/data_processing-1.0.0-py3-none-any.whl"
            - pypi:
                package: "pandas"
                version: "2.0.0"
          asset_specs:
            - key: processed_customer_data
              description: "Processed customer data with job parameter references"
              kinds: ["databricks", "notebook", "processing"]

        # Example 2: Python wheel task with job parameter references in list format
        - task_key: stage_documents
          depends_on:
            - task_key: data_processing_notebook
          python_wheel_task:
            package_name: "ml_etl"
            entry_point: "main"
            parameters:
              [
                "--run_id={{job.parameters.orchestrator_job_run_id}}",
                "--module=ml_etl.projects.tune.tasks.test_task",
                "--class=TestTask",
                "--environment={{job.parameters.environment}}",
                "--profiler_enabled={{job.parameters.profiler_enabled}}",
                "--experiment_settings={{job.parameters.experiment_settings}}",
                "--catalog_map={{job.parameters.catalog_map}}",
                "--log_level={{job.parameters.log_level}}",
                "--llm_calls={{job.parameters.llm_calls}}",
                "--git_sha={{job.parameters.git_sha}}",
                "--batch_size={{job.parameters.batch_size}}",
                "--timeout_minutes={{job.parameters.timeout_minutes}}",
                "--input_table={{ env }}.processed.customer_data",
                "--output_table={{ env }}.features.customer_features"
              ]
          libraries:
            - whl: "dbfs:/FileStore/wheels/ml_processing-2.1.0-py3-none-any.whl"
            - pypi:
                package: "scikit-learn"
                version: "1.3.0"
          asset_specs:
            - key: customer_features
              description: "Engineered customer features with job parameter references"
              kinds: ["databricks", "wheel", "features"]
              deps: ["processed_customer_data"]

        # Example 3: Spark JAR task with job parameter references
        - task_key: spark_processing_jar
          depends_on:
            - task_key: stage_documents
          spark_jar_task:
            main_class_name: "com.company.etl.DataProcessor"
            parameters:
              input_table: "{{ env }}.raw.transactions"
              output_table: "{{ env }}.processed.transactions"
              processing_date: "{{ ds }}"
              # Mix of direct values and job parameter references
              environment: "{{job.parameters.environment}}"
              run_id: "{{job.parameters.orchestrator_job_run_id}}"
              output_bucket: "{{job.parameters.output_bucket_name}}"
              profiler_enabled: "{{job.parameters.profiler_enabled}}"
              batch_size: "{{job.parameters.batch_size}}"
              timeout_minutes: "{{job.parameters.timeout_minutes}}"
              experiment_settings: "{{job.parameters.experiment_settings}}"
              catalog_map: "{{job.parameters.catalog_map}}"
              log_level: "{{job.parameters.log_level}}"
              llm_calls: "{{job.parameters.llm_calls}}"
              git_sha: "{{job.parameters.git_sha}}"
          libraries:
            - jar: "dbfs:/FileStore/jars/data_processor-1.0.0.jar"
            - maven:
                coordinates: "org.apache.spark:spark-streaming_2.12:3.4.0"
          asset_specs:
            - key: processed_transactions
              description: "Processed transaction data with job parameter references"
              kinds: ["databricks", "jar", "etl"]
              deps: ["customer_features"]

        # Example 4: Run existing job with job parameter references
        - task_key: existing_job_with_references
          depends_on:
            - task_key: spark_processing_jar
          run_job_task:
            job_id: 123456789
            job_parameters:
              input_table: "{{ env }}.processed.transactions"
              output_table: "{{ env }}.analytics.daily_summary"
              summary_date: "{{ ds }}"
              # Reference job-level parameters in job parameters
              environment: "{{job.parameters.environment}}"
              run_id: "{{job.parameters.orchestrator_job_run_id}}"
              output_bucket: "{{job.parameters.output_bucket_name}}"
              profiler_enabled: "{{job.parameters.profiler_enabled}}"
              experiment_settings: "{{job.parameters.experiment_settings}}"
              catalog_map: "{{job.parameters.catalog_map}}"
              log_level: "{{job.parameters.log_level}}"
              llm_calls: "{{job.parameters.llm_calls}}"
              git_sha: "{{job.parameters.git_sha}}"
              batch_size: "{{job.parameters.batch_size}}"
              timeout_minutes: "{{job.parameters.timeout_minutes}}"
          asset_specs:
            - key: daily_summary
              description: "Daily analytics summary with job parameter references"
              kinds: ["databricks", "job", "analytics"]
              deps: ["processed_transactions"]

        # Example 5: Condition task
        - task_key: check_data_quality
          depends_on:
            - task_key: existing_job_with_references
          condition_task:
            op: "GREATER_THAN"
            left: "{{ tasks.existing_job_with_references.outputs.records_processed }}"
            right: "{{job.parameters.batch_size}}"

    # Example 6: Another job with different job-level parameters
    analytics_job:
      name: "Analytics Job with Job Parameters"
      
      # Different job-level parameters for this job
      parameters:
        environment: "{{ env }}"
        orchestrator_job_run_id: "{{ run_id }}"
        analytics_mode: "advanced"
        cache_enabled: true
        max_retries: 3
        timeout_hours: 2
      
      tasks:
        - task_key: analytics_processing
          notebook_task:
            notebook_path: "/Users/{{ workspace_user }}/ml_pipelines/analytics"
            base_parameters:
              input_table: "{{ env }}.processed.transactions"
              output_table: "{{ env }}.analytics.results"
              # Reference job-level parameters
              environment: "{{job.parameters.environment}}"
              run_id: "{{job.parameters.orchestrator_job_run_id}}"
              analytics_mode: "{{job.parameters.analytics_mode}}"
              cache_enabled: "{{job.parameters.cache_enabled}}"
              max_retries: "{{job.parameters.max_retries}}"
              timeout_hours: "{{job.parameters.timeout_hours}}"
          asset_specs:
            - key: analytics_results
              description: "Analytics results with job parameter references"
              kinds: ["databricks", "notebook", "analytics"] 
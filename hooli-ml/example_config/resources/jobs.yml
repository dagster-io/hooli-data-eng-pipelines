# Sample jobs.yml file demonstrating new features
# This shows how the scaffolder extracts libraries, job parameters, and table locations

resources:
  jobs:
    ml_pipeline_job:
      name: "ML Pipeline Job"
      tasks:
        # Example 1: Notebook task with libraries and job parameters
        - task_key: data_processing_notebook
          notebook_task:
            notebook_path: "/Users/{{ workspace_user }}/ml_pipelines/data_processing"
            base_parameters:
              input_table: "{{ env }}.raw.customer_data"
              output_table: "{{ env }}.processed.customer_data"
              processing_date: "{{ ds }}"
              input_path: "/dbfs/data/raw/customers/"
              output_path: "/dbfs/data/processed/customers/"
              data_location: "s3://my-bucket/data/"
              source_table: "{{ env }}.raw.transactions"
              target_table: "{{ env }}.processed.transactions"
          libraries:
            - whl: "dbfs:/FileStore/wheels/data_processing-1.0.0-py3-none-any.whl"
            - pypi:
                package: "pandas"
                version: "2.0.0"
            - maven:
                coordinates: "org.apache.spark:spark-sql_2.12:3.4.0"
                repo: "https://repo1.maven.org/maven2/"
          job_parameters:
            - name: orchestrator_job_run_id
              default: "{{ run_id }}"
            - name: environment
              default: "{{ env }}"
            - name: output_bucket_name
              default: "{{ var.output_bucket_name }}"
            - name: profiler_enabled
              default: false
            - name: input_table_location
              default: "{{ env }}.raw.customer_data"
            - name: output_table_location
              default: "{{ env }}.processed.customer_data"

        # Example 2: Python wheel task with libraries and job parameters
        - task_key: feature_engineering_wheel
          depends_on:
            - task_key: data_processing_notebook
          python_wheel_task:
            package_name: "ml_processing_package"
            entry_point: "engineer_features"
            parameters:
              - input_path: "/dbfs/data/processed/"
              - output_path: "/dbfs/data/features/"
              - input_table: "{{ env }}.processed.customer_data"
              - output_table: "{{ env }}.features.customer_features"
              - feature_version: "v2.0"
              - source_location: "s3://my-bucket/processed/"
              - target_location: "s3://my-bucket/features/"
              - module: "custom.feature.engineering"
          libraries:
            - whl: "dbfs:/FileStore/wheels/ml_processing-2.1.0-py3-none-any.whl"
            - maven:
                coordinates: "org.apache.spark:spark-sql_2.12:3.4.0"
                repo: "https://repo1.maven.org/maven2/"
            - pypi:
                package: "scikit-learn"
                version: "1.3.0"
            - cran:
                package: "dplyr"
                repo: "https://cran.r-project.org"
          job_parameters:
            orchestrator_job_run_id: "{{ run_id }}"
            environment: "{{ env }}"
            output_bucket_name: "{{ var.output_bucket_name }}"
            profiler_enabled: false
            batch_size: 1000
            timeout_minutes: 30
            source_table_location: "{{ env }}.processed.customer_data"
            target_table_location: "{{ env }}.features.customer_features"

        # Example 3: Spark JAR task with libraries and job parameters
        - task_key: spark_processing_jar
          depends_on:
            - task_key: feature_engineering_wheel
          spark_jar_task:
            main_class_name: "com.company.etl.DataProcessor"
            parameters:
              - input_table: "{{ env }}.raw.transactions"
              - output_table: "{{ env }}.processed.transactions"
              - processing_date: "{{ ds }}"
              - input_path: "/dbfs/data/raw/transactions/"
              - output_path: "/dbfs/data/processed/transactions/"
              - source_location: "s3://my-bucket/raw/"
              - target_location: "s3://my-bucket/processed/"
              - batch_size: 1000
              - timeout_minutes: 30
          libraries:
            - jar: "dbfs:/FileStore/jars/data_processor-1.0.0.jar"
            - maven:
                coordinates: "org.apache.spark:spark-streaming_2.12:3.4.0"
            - maven:
                coordinates: "com.amazonaws:aws-java-sdk-s3:1.12.261"
                exclusions: ["com.fasterxml.jackson.core:jackson-core"]
          job_parameters:
            - name: orchestrator_job_run_id
              default: "{{ run_id }}"
            - name: environment
              default: "{{ env }}"
            - name: output_bucket_name
              default: "{{ var.output_bucket_name }}"
            - name: profiler_enabled
              default: false
            - name: spark_executor_memory
              default: "4g"
            - name: spark_driver_memory
              default: "2g"
            - name: source_table_location
              default: "{{ env }}.raw.transactions"
            - name: target_table_location
              default: "{{ env }}.processed.transactions"

        # Example 4: Run existing job with job parameters
        - task_key: existing_job_with_table_locations
          depends_on:
            - task_key: spark_processing_jar
          run_job_task:
            job_id: 123456789
            job_parameters:
              input_table: "{{ env }}.processed.transactions"
              output_table: "{{ env }}.analytics.daily_summary"
              summary_date: "{{ ds }}"
              input_path: "/dbfs/data/processed/transactions/"
              output_path: "/dbfs/data/analytics/summary/"
              source_location: "s3://my-bucket/processed/"
              target_location: "s3://my-bucket/analytics/"
              orchestrator_job_run_id: "{{ run_id }}"
              environment: "{{ env }}"
              output_bucket_name: "{{ var.output_bucket_name }}"
              profiler_enabled: false

        # Example 5: Condition task
        - task_key: check_data_quality
          depends_on:
            - task_key: existing_job_with_table_locations
          condition_task:
            op: "GREATER_THAN"
            left: "{{ tasks.existing_job_with_table_locations.outputs.records_processed }}"
            right: "1000" 